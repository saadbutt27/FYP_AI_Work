{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Using cached groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from groq) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from groq) (0.25.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from groq) (2.6.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from groq) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2023.11.17)\n",
      "Requirement already satisfied: httpcore in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.16.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\saad2\\appdata\\local\\anaconda3\\envs\\python12\\lib\\site-packages (from httpcore->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install python-dotenv\n",
    "%pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have gained significant importance in recent years due to their ability to quickly generate text, respond to user queries, and drive a variety of applications in various domains. Here are some reasons why they are important:\n",
      "\n",
      "1. **Speed and Efficiency**: Fast language models are designed to process and generate text quickly, making them ideal for real-time applications such as chatbots, customer service, and language translation. They can respond to user queries in a matter of seconds, reducing wait times and improving user experience.\n",
      "2. **Scalability**: With the increasing amount of data being generated every day, fast language models can handle large volumes of text data and generate responses quickly, making them suitable for large-scale applications.\n",
      "3. **Improved User Engagement**: Fast language models can engage users more effectively by providing quick and accurate responses to their queries. This can lead to higher user satisfaction, increased loyalty, and improved brand reputation.\n",
      "4. **Multi-Modal Interactions**: Fast language models can be integrated with various modalities such as speech, vision, and gesture recognition, enabling humans to interact with machines in a more natural and intuitive way.\n",
      "5. **AI-Powered Assistant**: Fast language models can be used to power AI-powered assistants, such as virtual assistants (e.g., Amazon Alexa, Google Assistant) and voice assistants (e.g., Apple Siri, Microsoft Cortana).\n",
      "6. **Content Generation**: Fast language models can generate content quickly, making them suitable for applications such as:\n",
      "\t* News articles and social media posts\n",
      "\t* Product descriptions and marketing materials\n",
      "\t* Chatbot interactions and customer support responses\n",
      "\t* Content creation for entertainment and education\n",
      "7. **Healthcare and Medical Applications**: Fast language models can be used in healthcare and medical applications to quickly process and analyze medical records, diagnose diseases, and provide personalized treatment plans.\n",
      "8. **Financial and Economic Applications**: Fast language models can be used in financial and economic applications to quickly analyze financial data, predict market trends, and provide personalized investment advice.\n",
      "9. **Improved Language Understanding**: Fast language models can improve language understanding by learning from vast amounts of text data and generating responses that are more accurate, coherent, and contextually relevant.\n",
      "10. **Advancements in Neuroscience and AI Research**: Fast language models have the potential to advance our understanding of human language processing, cognition, and artificial intelligence research, leading to breakthroughs in fields such as neuroscience, linguistics, and computer science.\n",
      "\n",
      "In summary, fast language models have the potential to transform various industries and aspects of our daily lives by providing quick, accurate, and engaging interactions with machines. Their ability to process and generate text quickly makes them an essential component of many AI-powered applications.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sentiment:** Positive\n",
      "**Confidence Score:** 0.95\n",
      "**Response:** \"It's wonderful to hear that you had a fantastic experience with our service. We're thrilled to have exceeded your expectations!\"\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a data analyst capable of sentiment analysis that responds in with these fields: sentiment(positive, negative, or neutral), confidence score(0-1), and a breif and concise response to cater that sentiment.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am absolutely thrilled with the excellent service I received today!\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama3-8b-8192\",\n",
    "\n",
    "    #\n",
    "    # Optional parameters\n",
    "    #\n",
    "\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 32,768 tokens shared between prompt and completion.\n",
    "    max_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=1,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    stop=None,\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sentiment:** Negative\n",
      "**Confidence Score:** 0.9\n",
      "**Response:** The product's subpar quality has left a sour taste, and I'm thoroughly unimpressed.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a data analyst capable of sentiment analysis that responds in with these fields: sentiment(positive, negative, or neutral), confidence score(0-1), and a breif and concise response to cater that sentiment.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am extremely disappointed with the quality of the product.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.5,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n",
      "Confidence Score: 0.8\n",
      "Response: \"It's great to see that the meeting started and ended on schedule, demonstrating good time management and respect for everyone's time.\"\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a data analyst capable of sentiment analysis that responds in with these fields: sentiment(positive, negative, or neutral), confidence score(0-1), and a breif and concise response to cater that sentiment.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The meeting was scheduled for 3 PM, and it ended on time.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.5,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat completion with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to explain how a decision tree works for classification, focusing on the concept of entropy and information gain.\n",
      "\n",
      "**Entropy**\n",
      "\n",
      "In information theory, entropy measures the amount of uncertainty or randomness in a probability distribution. In the context of decision trees, entropy is used to quantify the impurity or uncertainty in the class labels of a node. The goal is to split the node into two child nodes that minimize the entropy.\n",
      "\n",
      "Entropy is calculated using the following formula:\n",
      "\n",
      "H(Y) = - ∑ (p(y) * log2(p(y)))\n",
      "\n",
      "where H(Y) is the entropy of the class labels, p(y) is the probability of each class label, and the sum is taken over all class labels.\n",
      "\n",
      "**Information Gain**\n",
      "\n",
      "Information gain measures the reduction in entropy after splitting a node based on a particular feature. It's calculated as the difference between the entropy of the parent node and the weighted average of the entropies of the child nodes.\n",
      "\n",
      "IG(X, Y) = H(Y) - ∑ (p(y) * H(Y|X=x))\n",
      "\n",
      "where IG(X, Y) is the information gain, H(Y) is the entropy of the parent node, p(y) is the probability of each class label, and H(Y|X=x) is the entropy of the child nodes given the feature X=x.\n",
      "\n",
      "**How Decision Trees Work**\n",
      "\n",
      "A decision tree works by recursively partitioning the data into smaller subsets based on the feature that maximizes the information gain. The process starts with the root node, which contains the entire dataset.\n",
      "\n",
      "1. Calculate the entropy of the root node.\n",
      "2. Choose the feature that maximizes the information gain.\n",
      "3. Split the node into two child nodes based on the chosen feature.\n",
      "4. Recursively repeat steps 1-3 for each child node until a stopping criterion is met (e.g., all instances in a node belong to the same class).\n",
      "\n",
      "**Real-World Example**\n",
      "\n",
      "Let's consider a classification problem where we want to predict whether a customer will churn based on their demographic and usage data. We have the following dataset:\n",
      "\n",
      "| Customer ID | Age | Income | Plan Type | Churn |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| 1 | 30 | 50000 | Postpaid | No |\n",
      "| 2 | 40 | 70000 | Prepaid | Yes |\n",
      "| 3 | 25 | 30000 | Postpaid | No |\n",
      "| 4 | 50 | 90000 | Prepaid | Yes |\n",
      "| 5 | 35 | 40000 | Postpaid | No |\n",
      "| 6 | 45 | 60000 | Prepaid | Yes |\n",
      "\n",
      "We start with the root node containing all 6 customers.\n",
      "\n",
      "1. Calculate the entropy of the root node: H(Y) = - (2/6 * log2(2/6) + 4/6 * log2(4/6)) ≈ 0.9709\n",
      "2. Calculate the information gain for each feature:\n",
      "\t* Age: IG(Age, Y) = 0.9709 - (2/6 * H(Y|Age=25) + 2/6 * H(Y|Age=30) + 1/6 * H(Y|Age=35) + 1/6 * H(Y|Age=40)) ≈ 0.2341\n",
      "\t* Income: IG(Income, Y) = 0.9709 - (1/6 * H(Y|Income=30000) + 2/6 * H(Y|Income=40000) + 2/6 * H(Y|Income=50000) + 1/6 * H(Y|Income=60000)) ≈ 0.4364\n",
      "\t* Plan Type: IG(Plan Type, Y) = 0.9709 - (3/6 * H(Y|Plan Type=Postpaid) + 3/6 * H(Y|Plan Type=Prepaid)) ≈ 0.5141\n",
      "3. Choose the feature that maximizes the information gain: Plan Type (IG = 0.5141)\n",
      "4. Split the node into two child nodes based on the chosen feature:\n",
      "\t* Child node 1: Plan Type = Postpaid (customers 1, 3, and 5)\n",
      "\t* Child node 2: Plan Type = Prepaid (customers 2, 4, and 6)\n",
      "5. Recursively repeat the process for each child node until a stopping criterion is met.\n",
      "\n",
      "The resulting decision tree would look like this:\n",
      "\n",
      "```\n",
      "          Root Node\n",
      "         /         \\\n",
      "    Plan Type = Postpaid  Plan Type = Prepaid\n",
      "         /           \\\n",
      "  Customer ID = 1, 3, 5  Customer ID = 2, 4, 6\n",
      "```\n",
      "\n",
      "In this example, the decision tree splits the data based on the plan type feature, which provides the highest information gain. The tree continues to split the data based on the feature that maximizes theNone"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a machine learning instructor skilled in advanced algorithms. Provide concise explanations with practical examples, focusing on complex topics like decision trees, gradient boosting, and neural networks.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you explain how a decision tree works for classification, specifically focusing on the concept of entropy and information gain? Provide a real-world example to illustrate how these are used to split nodes in the tree.\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama3-8b-8192\",\n",
    "\n",
    "    #\n",
    "    # Optional parameters\n",
    "    #\n",
    "\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 2048 tokens shared between prompt and completion.\n",
    "    max_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=1,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    stop=None,\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Print the incremental deltas returned by the LLM.\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat completion with stop sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Count to 10. Your response must begin with \\\"1, \\\".  example: 1, 2, 3, ...\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama3-8b-8192\",\n",
    "\n",
    "    #\n",
    "    # Optional parameters\n",
    "    #\n",
    "\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 2048 tokens shared between prompt and completion.\n",
    "    max_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=1,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    # For this example, we will use \", 6\" so that the llm stops counting at 5.\n",
    "    # If multiple stop values are needed, an array of string may be passed,\n",
    "    # stop=[\", 6\", \", six\", \", Six\"]\n",
    "    stop=\", 6\",\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
